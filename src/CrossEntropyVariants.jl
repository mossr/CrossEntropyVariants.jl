"""
Cross-entropy method variants for optimization.
    - Cross-entropy method (standard)
    - Cross-entropy surrogate method
    - Cross-entropy mixture method

See paper for full description:
    http://web.stanford.edu/~mossr/pdf/cem_variants.pdf
"""
module CrossEntropyVariants

export ce_surrogate,
       ce_mixture,
       cross_entropy_method,
       sierra,
       branin,
       ackley,
       paraboloid,
       PlotSettings


using Distributions
using DataStructures
using GaussianMixtures
using GaussianProcesses
using Parameters
using Optim
using StatsBase
using Random
using Colors
using ColorSchemes
using PGFPlots
using PyPlot
using LinearAlgebra
using Suppressor

const â„ = Float64
const â„¤ = Int64

include("utils.jl")
include("sierra.jl")
include("test_objective_functions.jl")
include("surrogate_models.jl")
include("evaluation_schedules.jl")
include("plotting.jl")

dist_manhattan(ğ¯, ğ¯â€²) = norm(ğ¯ - ğ¯â€², 1)
dist_euclidean(ğ¯, ğ¯â€²) = norm(ğ¯ - ğ¯â€², 2)
dist_supremum(ğ¯, ğ¯â€²) = norm(ğ¯ - ğ¯â€², Inf)

function nearest_neighbors(xâ€², ğ’Ÿ, dist)
    ğ’Ÿ[argmin([dist(x, xâ€²) for (x,y) âˆˆ ğ’Ÿ])][2]
end

import Distributions.MvNormal
function MvNormal(mu::Vector{Int}, Sigma::Matrix{Int})
    # Fix PDMat errors when Sigma is an integer matrix
    # Temporarty fix: https://github.com/JuliaStats/PDMats.jl/pull/117
    return MvNormal(mu, convert(Matrix{Float64}, Sigma))
end

"""
    The cross-entropy surrogate method.
"""
function ce_surrogate(f, ğŒ; # objective function `f` and initial distribution `ğŒ`
                      m=10, # Number of samples per iteration
                      m_elite=5, # Number of elite samples
                      Î±=1, # Covariance scale for model-elites
                      k_max=10, # Maximum number of iterations
                      Ïµ=1e-9, # Change in objective stopping condition (unused)
                      plot=false, # Plotting indicator
                      redraw=false, # Redraw plot every time
                      p_geo=NaN, # Evaluation scheduling parameter for geometric distribution
                      plot_settings=PlotSettings(), # Plotting keywork arguments (see `plotting`)
                      surrogate_kwargs...) # Surrogate model parameters (see `surrogate_model`)
    plot ? initial_plot(f) : nothing
    ğ’Ÿ = Queue{Vector}() # population data

    báµ¥ = Inf
    bâ‚“ = missing
    beáµ¥ = Inf # Best estimate value (from surrogate model).
    beâ‚“ = missing # Best estimate x-value (from surrogate model).
    Î£ = deepcopy(isa(ğŒ, MixtureModel) ? ğŒ.components[1].Î£ : ğŒ.Î£)

    if isnan(p_geo)
        use_evaluation_schedule = false
    else
        use_evaluation_schedule = true
        Pâ‚› = truncated(Geometric(p_geo), 0, k_max) # for evaluation schedule.
        Int(Random.GLOBAL_RNG.seed[1]) == 1 ? @info("Evaluation schedule: $(map(k->evaluation_schedule(Pâ‚›, k, k_max, m, m_elite)[1], 1:k_max))") : nothing
    end

    for k in 1:k_max
        if use_evaluation_schedule
            mâ‚‘, m_elite = evaluation_schedule(Pâ‚›, k, k_max, m, m_elite)
        else
            mâ‚‘ = m
        end

        samples = rand(ğŒ, mâ‚‘)
        Y = [f(samples[:,i]) for i in 1:mâ‚‘]
        order = sortperm(Y)
        elite = samples[:,order[1:m_elite]]

        ğ„, ğŒ, ğ’®, model_elite = model_elite_set!(ğ’Ÿ, samples, elite, Y, ğŒ, Î£, m, m_elite, Î±, k; surrogate_kwargs...)

        plot ? plotting(k, missing, ğŒ, ğ’®, missing, m_elite, samples, model_elite, elite, true; settings=plot_settings) : nothing

        ğŒ = fit(ğŒ, ğ„)

        # Monitoring. Non-algorithmic.
        if length(order) > 1 # For the cases where evaluation_schedule returned 0
            yâ‚œ = Y[order[1]] # Top elite y value.
            if yâ‚œ < báµ¥
                báµ¥ = yâ‚œ # Better value.
                bâ‚“ = samples[:,order[1]]
            end
        end

    end

    return ğŒ, bâ‚“, báµ¥
end


"""
Model sub-component elite set.
    â„“: short-term memory length
"""
function model_elite_set!(ğ’Ÿ, samples, elite, Y, ğŒ, Î£, m, m_elite, Î±, k; â„“=3, surrogate_kwargs...)
    # Surrogate model: Gaussian process.
    ğ’Ÿâ‚š = map(tuple, columns(samples), Y) # fit to entire population

    # Use short-term memory
    enqueue!(ğ’Ÿ, ğ’Ÿâ‚š)
    while length(ğ’Ÿ) > â„“
        dequeue!(ğ’Ÿ)
    end
    ğ’Ÿ = reduce(vcat, ğ’Ÿ) # Re-cast, abuse of variable notation
    ğ’®hat = surrogate_model(ğ’Ÿ; surrogate_kwargs...)

    m_model = 10m
    m_model_elite = 10m_elite
    model_samples = rand(ğŒ, m_model)
    model_Y = [ğ’®hat(model_samples[:,i]) for i in 1:(m_model)]
    model_order = sortperm(model_Y)
    model_elite = model_samples[:,model_order[1:(m_model_elite)]]

    # Elites form their own distributions based on the surrogate model.
    cov = Î£/Î±
    ğ¦ = map(e->MvNormal(e, cov), columns(elite))

    ğ„ = Matrix{â„}(undef, 2, 0) # Elite set.

    # For each elite in ğ¦, run CE-method to completion to choose new "sub-elites" (use Î¼ = eâ‚“ and Î£ = M.Î£)
    sub_elite = Matrix{â„}(undef, 2, 0) # Sub-elite set.
    for i in 1:length(ğ¦)
        # Note. If this k_max is too large, then we could converge/overfit to the surrogate model
        # (especially when the outer `k` is low, the surrogate model has not matured yet)
        ğ¦[i], cem_bâ‚“, cem_báµ¥ = cross_entropy_method(ğ’®hat, ğ¦[i]; m=100,
                                                                 m_elite=10,
                                                                 k_max=2)
        # Add top elite, not the mean.
        sub_elite = hcat(sub_elite, cem_bâ‚“)
    end

    if isa(ğŒ, MixtureModel)
        # Mix ğ¦ into ğŒ (mixture)
        ğŒ = MixtureModel(ğ¦)
    end

    ğ„ = hcat(ğ„, elite, model_elite, sub_elite)
    return ğ„, ğŒ, ğ’®hat, model_elite
end


"""
Cross-entropy mixture method. Same as `ce_surrogate` but using mixture models.
"""
function ce_mixture(f, ğŒ; kwargs...)
    ğŒ = MixtureModel([ğŒ])
    @show ğŒ
    return ce_surrogate(f, ğŒ; kwargs...)
end



import Distributions.fit
fit(ğŒ::MixtureModel, ğ„; kwargs...) = fit(GMM(ğŒ), ğ„; kwargs...) # Cast input ğŒ to GMM
"""
Gaussian mixture model fitting using the Expectation Maximization algorithm.
"""
function fit(ğŒ::GMM, ğ„)
    try
        # Expectation maximization to fit mixture model ğŒ to elite set (data) ğ„
        @suppress em!(ğŒ, permutedims(ğ„))
    catch err
        @warn err
    end

    return MixtureModel(ğŒ) # re-cast
end

"""
Fit the multivariate Normal distribution ğŒ to data set ğ„.
"""
function fit(ğŒ::MvNormal, ğ„)
    try
        ğŒ = fit(typeof(ğŒ), ğ„)
    catch err
        @warn err
    end

    return ğŒ
end



# Kochenderfer and Wheeler, Algorithms for Optimization, Algorithm 8.7, Page 135
"""
    The cross-entropy method, which takes an objective
    function `f` to be minimized, a proposal distribution
    `P`, an iteration count `k_max`, a sample size `m`, and
    the number of samples to use when refitting the
    distribution `m_elite`. It returns the updated distribution
    over where the global minimum is likely to exist.
"""
function cross_entropy_method(f, P; m=10, m_elite=5, k_max=10, Ïµ=1e-9, plot=false, redraw=false, p_geo=nothing)
    báµ¥ = Inf
    bâ‚“ = missing
    plot ? initial_plot(f) : nothing
    Î¼ = mean(P)

    for k in 1:k_max
        samples = rand(P, m)
        Y = [f(samples[:,i]) for i in 1:m]
        order = sortperm(Y)
        elite = samples[:,order[1:m_elite]]

        # Top elite.
        yâ‚œ = Y[order[1]] # Top elite y value
        if yâ‚œ < báµ¥
            báµ¥ = yâ‚œ # Found better. (Monitoring only)
            bâ‚“ = samples[:,order[1]]
        end

        plot ? plot_cem(P, samples, order, m_elite) : nothing

        try
            P = fit(typeof(P), elite)
        catch err
            @warn err
            @info k
        end

        Î¼ = mean(P)
    end

    return (P, bâ‚“, báµ¥)
end


end # module CrossEntropyVariants